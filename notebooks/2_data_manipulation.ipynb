{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src import string_handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before matching the [USA Spending](https://www.usaspending.gov/#/) data to the data of Chair of Financial Management and Capital Markets (which will be called the _\"chair data\"_ from now on), we need to manipulate both datasets to make their fields compatible with each other.\n",
    "\n",
    "For this purpose, we have implemented handler methods. We will apply these methods to both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA Spending Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first notebook, we combined all files into a single .csv file using `csv_handlers.extract_all_recipients()` after downloading the data from [USA Spending](https://www.usaspending.gov/#/).\n",
    "\n",
    "Now, we will load this .csv file and process the data with the help of the `string_handlers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df = pd.read_csv(\"../data/all_recipients.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what we have in our combined DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recipient_duns                       object\n",
       "recipient_name                       object\n",
       "recipient_doing_business_as_name     object\n",
       "recipient_parent_duns                object\n",
       "recipient_parent_name                object\n",
       "recipient_country_code               object\n",
       "recipient_country_name               object\n",
       "recipient_address_line_1             object\n",
       "recipient_address_line_2             object\n",
       "recipient_city_name                  object\n",
       "recipient_state_code                 object\n",
       "recipient_state_name                 object\n",
       "recipient_zip_4_code                 object\n",
       "recipient_congressional_district     object\n",
       "recipient_phone_number               object\n",
       "recipient_fax_number                 object\n",
       "recipient_city_code                  object\n",
       "recipient_county_code               float64\n",
       "recipient_county_name                object\n",
       "recipient_zip_code                  float64\n",
       "recipient_zip_last_4_code           float64\n",
       "recipient_foreign_city_name          object\n",
       "recipient_foreign_province_name      object\n",
       "recipient_foreign_postal_code        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating a list of unique names in our dataset. These names are taken from `recipient_name`, `recipient_parent_name`, and `recipient_doing_business_as_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2860451 unique names.\n"
     ]
    }
   ],
   "source": [
    "uniq_names = np.concatenate([usa_df[\"recipient_name\"].dropna().unique(),\n",
    "                             usa_df[\"recipient_parent_name\"].dropna().unique(),\n",
    "                             usa_df[\"recipient_doing_business_as_name\"].dropna().unique()\n",
    "                            ])\n",
    "\n",
    "uniq_names = np.unique(uniq_names)\n",
    "print(f\"We have {len(uniq_names)} unique names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will process these names. While doing this, we will always keep track of (old name, new name) pairs, so that we can match the new names with old names without worrying about from which column (recipient_name/parent_name/doing_business_as_name) the name came."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run two methods on the names.\n",
    "\n",
    "__1) Letter-level processing:__ Here, we try to normalize the letters in the dataset. This consists of:\n",
    "* cleaning whitespace between ampersand (&) and short neighboring words, e.g. \"AT & T\" -> \"AT&T\"\n",
    "* cleaning whitespace between period (.) and short neighboring words, e.g. \"U. S. A.\" -> \"U.S.A\"\n",
    "* replace ampersand (&) with \"AND\", if it is between two long words, e.g. \"ZWERK & SONS FARMS\" -> \"ZWERK AND SONS FARMS\"\n",
    "* getting rid of uncommon and unnecessary characters, e.g. \"INFINITEÂ¦ABM, LLC\" -> \"INFINITE ABM LLC\"\n",
    "* getting rid of continuous multiple whitespaces, e.g. \"U P COMMUNITY SERVICES, INC.\" -> \"UP COMMUNITY SERVICES, INC.\"\n",
    "\n",
    "__2) Word-level processing:__ Here, we abbreviate all instances of words that are frequently abbreviated. For instance, \"incorporated\" is commonly abbreviated as \"inc\". By replacing all \"incorporated\"s with \"inc\"s, we increase our matching rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a record of old names\n",
    "old_new_names = pd.DataFrame(uniq_names, columns=[\"old\"])\n",
    "\n",
    "# process names\n",
    "uniq_names = string_handlers.fix_letters(uniq_names)\n",
    "uniq_names = string_handlers.fix_words(uniq_names)\n",
    "\n",
    "# match each old name with its new name\n",
    "old_new_names[\"new\"] = uniq_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our (old, new) name pairs, we can add columns to our dataset, representing the \"clean\" versions of `recipient_name`, `recipient_parent_name`, and `recipient_doing_business_as_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df['clean_recipient_name'] = pd.merge(usa_df[\"recipient_name\"], old_new_names, right_on=\"old\",\n",
    "                                          left_on = \"recipient_name\", how=\"left\")[\"new\"].values\n",
    "\n",
    "usa_df['clean_recipient_parent_name'] = pd.merge(usa_df[\"recipient_parent_name\"], old_new_names, right_on=\"old\",\n",
    "                                                 left_on = \"recipient_parent_name\", how=\"left\")[\"new\"].values\n",
    "\n",
    "usa_df['clean_recipient_doing_business_as_name'] = pd.merge(usa_df[\"recipient_doing_business_as_name\"], \n",
    "                                                            old_new_names, right_on=\"old\",\n",
    "                                                            left_on = \"recipient_doing_business_as_name\",\n",
    "                                                            how=\"left\")[\"new\"].values\n",
    "usa_df.loc[usa_df.recipient_parent_duns.isna(), 'clean_recipient_parent_name'] = np.nan\n",
    "usa_df.loc[usa_df.recipient_duns.isna(), 'clean_recipient_name'] = np.nan\n",
    "usa_df.loc[usa_df.recipient_duns == usa_df.recipient_parent_duns, 'clean_recipient_parent_name'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to increase matching rate, and to identify better matches among multiple candidates, we use address similarity.\n",
    "\n",
    "Address similarity is calculated on different levels. Street names, zipcodes and state codes are used in this calculation. We implemented methods to standardize street name and state name representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_cols = [\"recipient_address_line_1\", \"recipient_address_line_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df[\"recipient_address_line_fixed\"] = string_handlers.fix_addresses(usa_df, addr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df[\"recipient_state_fixed\"] = usa_df[\"recipient_state_code\"].map(string_handlers.fix_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data manipulation is complete. We can now save the DataFrame so  that it can be used in the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df.to_csv(\"../processed/processed_usa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chair Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the USA Spending data, we read, process and save the chair data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_df = pd.read_excel(\"../data/company_dataset_identifier.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = string_handlers.fix_letters(chair_df[\"conm\"])\n",
    "chair_df[\"clean_conm\"]= string_handlers.fix_words(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_cols = [\"add1\",\"add2\",\"add3\",\"add4\"]\n",
    "chair_df[\"add_fixed\"] = string_handlers.fix_addresses(chair_df, addr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_df[\"state_fixed\"] = chair_df[\"state\"].map(string_handlers.fix_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_df.to_csv(\"../processed/processed_chair.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing we continue with [matching names](./3_matching_names.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
